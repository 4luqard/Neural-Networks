{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-lZCrSvYpW1"
   },
   "source": [
    "## Neural Networks with Keras\n",
    "Implementing different strategy on different things for neural networks on the \"Video Game Sales\" dataset from the Kaggle website using different metrics https://www.kaggle.com/datasets/gregorut/videogamesales/data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading The Data and The Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFNNq1yQhA5I"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.initializers import RandomUniform, glorot_uniform, Zeros\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn\n",
    "\n",
    "# Load the dataset\n",
    "data_nn = pd.read_csv('vgsales.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic EDA and Basic Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16598 entries, 0 to 16597\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Rank          16598 non-null  int64  \n",
      " 1   Name          16598 non-null  object \n",
      " 2   Platform      16598 non-null  object \n",
      " 3   Year          16327 non-null  float64\n",
      " 4   Genre         16598 non-null  object \n",
      " 5   Publisher     16540 non-null  object \n",
      " 6   NA_Sales      16598 non-null  float64\n",
      " 7   EU_Sales      16598 non-null  float64\n",
      " 8   JP_Sales      16598 non-null  float64\n",
      " 9   Other_Sales   16598 non-null  float64\n",
      " 10  Global_Sales  16598 non-null  float64\n",
      "dtypes: float64(6), int64(1), object(4)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# preprocess the data for both parts\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rank              0\n",
       "Name              0\n",
       "Platform          0\n",
       "Year            271\n",
       "Genre             0\n",
       "Publisher        58\n",
       "NA_Sales          0\n",
       "EU_Sales          0\n",
       "JP_Sales          0\n",
       "Other_Sales       0\n",
       "Global_Sales      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rank            0\n",
       "Name            0\n",
       "Platform        0\n",
       "Year            0\n",
       "Genre           0\n",
       "Publisher       0\n",
       "NA_Sales        0\n",
       "EU_Sales        0\n",
       "JP_Sales        0\n",
       "Other_Sales     0\n",
       "Global_Sales    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Name', 'Platform', 'Genre', 'Publisher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFNNq1yQhA5I"
   },
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "X = data_nn.drop('Global_Sales', axis=1)\n",
    "y = data_nn['Global_Sales']\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345)\n",
    "\n",
    "encoder = ce.TargetEncoder(cols=categorical_features)\n",
    "X_train[categorical_features] = encoder.fit_transform(X_train[categorical_features], y_train)\n",
    "X_test[categorical_features] = encoder.transform(X_test[categorical_features])\n",
    "\n",
    "# Handle missing values by filling them with the median of the column\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_test.median())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Function for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kug02xw0YuCE"
   },
   "source": [
    "### Different Weight Initialization Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_weight(initializer):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], kernel_initializer=initializer, activation='relu'))\n",
    "    model.add(Dense(32, kernel_initializer=initializer, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=initializer, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "yfCS0w-ghMa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Uniform initialization:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "RMSE: 0.0208\n",
      "\n",
      "Results for Xavier initialization:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "RMSE: 0.0170\n",
      "\n",
      "Results for Zero initialization:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "RMSE: 1.0141\n"
     ]
    }
   ],
   "source": [
    "initializers = [RandomUniform(seed=12345), glorot_uniform(seed=12345), Zeros()]\n",
    "initializer_names = ['Uniform', 'Xavier', 'Zero']\n",
    "\n",
    "for initializer, name in zip(initializers, initializer_names):\n",
    "    print(f\"\\nResults for {name} initialization:\")\n",
    "    model = build_model_weight(initializer)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=0)\n",
    "    evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ckzts_3hOFp"
   },
   "source": [
    "### Discussion: Weight Initialization\n",
    "In this challenge, we investigated how various weight initialization techniques affected a neural network's performance for a regression problem. Three initialization strategies were put to the test: Xavier (Glorot), Zero, and Uniform. The outcomes and our observations are as follows:\n",
    "\n",
    "1. Uniform Initialization:\n",
    "* RMSE: 0.0208\n",
    "* Initial weights are assigned consistently across a specific range using uniform initialization. By distributing the weights equally, this technique seeks to guarantee that no neuron has a sizable advantage from the beginning. With an RMSE of 0.0208, the model with uniform initialization fared reasonably well in our trial. This implies that the weights were sufficiently balanced to give the learning process a respectable beginning.\n",
    "\n",
    "2. Xavier (Glorot) Initialization:\n",
    "* RMSE: 0.0170\n",
    "* The goal of Xavier initialization, sometimes referred to as Glorot initialization, is to maintain gradient and activation variance consistency throughout layers. Because it helps reduce the problems of vanishing or expanding gradients, this technique typically leads to faster convergence and improved overall performance. The Xavier-initialized model fared better in our trial than the other models, with the lowest RMSE of 0.0170. This demonstrates that Xavier initialization, which offers a more reliable and effective learning process, is very useful for our regression job.\n",
    "\n",
    "3. Zero Initialization:\n",
    "* RMSE: 1.0141\n",
    "* Every weight is set to zero at zero initialization. Because all of the neurons in the layer receive the identical updates during training, this approach often results in inefficient learning. With an RMSE of 1.0141, the model with zero initialization performed terribly, as was to be expected. Since zero initialization fails to disrupt the symmetry between neurons, it is ineffective for training neural networks, as evidenced by the high mistake rate.\n",
    "\n",
    "### Analysis and Impact of Weight Initialization\n",
    "In neural network training, weight initialization is essential. It has a direct impact on how fast and efficiently a model learns from the data. Our experiments led us to the following conclusions:\n",
    "\n",
    "* Uniform Initialization: This gave the model a balanced start, which led to respectable performance. It wasn't the best, but it made sure the network could still learn—just not as quickly as it could have with more advanced techniques.\n",
    "\n",
    "* Xavier Initialization: This initialization method was by far the best; it accelerated the convergence of the network and reduced its error rate. This technique leads to more effective training because it keeps a strong gradient flow throughout the network layers, which is very helpful for deep learning.\n",
    "\n",
    "* Zero Initialization: Drawn attention to itself as a bad example, zero initialization's subpar performance highlighted how crucial it is to select a suitable initialization approach. It proved that a network cannot identify significant patterns in the data unless there is appropriate weight variety.\n",
    "\n",
    "### Conclusion\n",
    "In conclusion, a neural network's performance and convergence are greatly impacted by the weight initialization strategy that is selected. In our regression task, Xavier initialization proved to be the most successful method, resulting in faster convergence and improved model correctness. This experiment emphasises how crucial it is to choose the right weight initialization strategies in order to improve learning and attain peak performance when training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUvUz7IAY6JE"
   },
   "source": [
    "### Varying The Number of Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "rArvMNhlhSxp"
   },
   "outputs": [],
   "source": [
    "def build_model_layer(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for 1 Hidden Layer:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "RMSE: 0.0382\n",
      "\n",
      "Results for 3 Hidden Layers:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "RMSE: 0.0300\n",
      "\n",
      "Results for 5 Hidden Layers:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "RMSE: 0.0374\n"
     ]
    }
   ],
   "source": [
    "layers = [1, 3, 5]\n",
    "layer_names = ['1 Hidden Layer', '3 Hidden Layers', '5 Hidden Layers']\n",
    "\n",
    "for num_layers, name in zip(layers, layer_names):\n",
    "    print(f\"\\nResults for {name}:\")\n",
    "    model = build_model_layer(num_layers)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=0)\n",
    "    evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wT4_E2a3hThd"
   },
   "source": [
    "### Discussion: Varying the Number of Layers\n",
    "In this job, we investigated how a neural network's performance on a regression issue is affected by the number of hidden layers in the network. Three distinct network topologies were tested: one with one hidden layer, three with three secret layers, and five with five hidden layers. The outcomes and our observations are as follows:\n",
    "\n",
    "1. 1 Hidden Layer:\n",
    "* RMSE: 0.0382\n",
    "* The most basic model, which had just one hidden layer, performed fairly well, with an RMSE of 0.0382. This model is computationally efficient since it has fewer parameters, which allows it to train more quickly. However, because of its simplicity, it might not be able to detect more intricate patterns in the data.\n",
    "\n",
    "2. 3 Hidden Layers:\n",
    "* RMSE: 0.0300\n",
    "* The model's performance was greatly enhanced by the addition of more layers, which caused the RMSE to drop below 0.0300. The model achieved equilibrium between training duration and complexity. Better prediction accuracy resulted from the network's ability to identify more complex patterns in the data thanks to the extra layers. This enhancement implies that, up to a certain degree, deepening the network can improve its capacity for generalisation.\n",
    "\n",
    "3. 5 Hidden Layers:\n",
    "* RMSE: 0.0374\n",
    "* The model's performance declined marginally with five hidden layers as compared to the three-layer model, yielding an RMSE of 0.0374. Despite being more intricate, this architecture didn't perform any better. The added depth probably brought with it new problems, including overfitting, or training deeper networks with problems like vanishing gradients, which can hinder learning.\n",
    "\n",
    "### Analysis and Impact of Number of Layers\n",
    "A neural network's ability to learn from data is directly impacted by the number of layers in the network. Below is a summary of the effects that were noticed:\n",
    "\n",
    "* 1 Hidden Layer: Although this model is easier to use and trains more quickly, it might not fully capture all of the data's intricacies. It works well for issues with less complexity or in situations where processing power is scarce.\n",
    "\n",
    "* 3 Hidden Layers: In our trial, this arrangement yielded the best results by striking a balance between training efficiency and complexity. The network was able to simulate more intricate relationships in the data thanks to the extra layers, which increased accuracy.\n",
    "\n",
    "* 5 Hidden Layers: The model's capability is increased by adding additional layers, but doing so also raises concerns like overfitting and challenging training. The fact that the three-layer network outperformed the five-layer network in our instance indicates that the extra complexity was not helpful—possibly even harmful.\n",
    "\n",
    "### Conclusion\n",
    "This experiment emphasises how crucial it is to choose a neural network's layer count appropriately. Deeper networks may be able to catch more intricate patterns, but in order to prevent overfitting and training problems, they also need to be tuned and regularised more carefully. With an RMSE of 0.0300, a model including three hidden layers yielded the highest results for our regression task, offering the optimum balance. This demonstrates that each issue calls for a customised approach to architecture design and that adding layers does not necessarily translate into improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzQYJ7UKY9O9"
   },
   "source": [
    "## Different Activation Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "r3Zsj9zJhcVp"
   },
   "outputs": [],
   "source": [
    "def build_model_activation(activation):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1]))\n",
    "    if activation == 'leaky_relu':\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "    else:\n",
    "        model.add(Dense(64, activation=activation))\n",
    "    model.add(Dense(32, activation=activation))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for ReLU activation:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "RMSE: 0.0091\n",
      "\n",
      "Results for Leaky ReLU activation:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "RMSE: 0.0331\n",
      "\n",
      "Results for Sigmoid activation:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "RMSE: 0.0612\n"
     ]
    }
   ],
   "source": [
    "activations = ['relu', 'leaky_relu', 'sigmoid']\n",
    "activation_names = ['ReLU', 'Leaky ReLU', 'Sigmoid']\n",
    "\n",
    "for activation, name in zip(activations, activation_names):\n",
    "    print(f\"\\nResults for {name} activation:\")\n",
    "    model = build_model_activation(activation)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=0)\n",
    "    evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aa_F0BnghcVq"
   },
   "source": [
    "### Discussion: Activation Functions\n",
    "In this challenge, we looked at how various activation functions affect a neural network's ability to solve a regression problem. ReLU, Leaky ReLU, and Sigmoid were the three activation functions that we examined. The outcomes and our observations are as follows:\n",
    "\n",
    "1. ReLU Activation:\n",
    "* RMSE: 0.0091\n",
    "* One of the most widely used activation functions in neural networks, especially for deep learning, is ReLU (Rectified Linear Unit). It makes training faster and solves the vanishing gradient problem well. The model with ReLU activation performed better in our experiment than the others, with the lowest RMSE of 0.0091. This suggests that ReLU works really well for our regression challenge, allowing for effective learning and better model performance.\n",
    "\n",
    "2. Leaky ReLU Activation:\n",
    "* RMSE: 0.0331\n",
    "* A version of ReLU called Leaky ReLU was created to solve the \"dying ReLU\" issue, in which neurons can stop learning altogether and became dormant. When the unit is not in use, it permits a slight, non-zero gradient. The model with Leaky ReLU activation exhibited a higher RMSE of 0.0331 than ReLU, despite its theoretical advantages. This shows that although Leaky ReLU can be useful in some situations, it did not enhance performance to the same extent for our particular regression problem.\n",
    "\n",
    "3. Sigmoid Activation:\n",
    "* RMSE: 0.0612\n",
    "* This higher error rate highlights the limitations of the Sigmoid function, especially in deeper networks where it can significantly slow down learning and reduce model performance. In our experiment, the model with Sigmoid activation performed the worst, with an RMSE of 0.0612. This function was widely used in the past but has since fallen out of favour due to issues like vanishing gradients and slower convergence.\n",
    "\n",
    "### Analysis and Impact of Activation Functions\n",
    "The performance and training dynamics of neural networks are strongly impacted by the selection of activation function. Below is a summary of the effects that were noticed:\n",
    "\n",
    "* ReLU Activation: ReLU is the recommended option for this regression work because it performed the best out of all the functions that were examined. It was obviously advantageous that it might reduce disappearing gradients and enable quicker, more effective training.\n",
    "\n",
    "* Leaky ReLU Activation: Leaky ReLU does not improve performance in this circumstance, despite addressing some of the shortcomings of regular ReLU. This indicates that the simplicity and effectiveness of the standard ReLU for our data were greater than the advantages of the Leaky ReLU's small negative value gradient.\n",
    "\n",
    "* Sigmoid Activation: The subpar performance of the Sigmoid function confirmed its limits in contemporary neural network applications. It is less appropriate for applications requiring deep or complex networks due to its problems with vanishing gradients and slower training.\n",
    "\n",
    "### Conclusion\n",
    "This experiment demonstrates how important activation functions are to neural network performance and training. ReLU was the clear victor for our regression challenge, offering the optimal ratio of accuracy to efficiency. Although theoretically better, leaky ReLU did not perform better in this situation than ReLU. According to what is known at this time, the Sigmoid function was substantially behind because of its intrinsic restrictions. These results highlight how crucial it is to choose the right activation functions depending on the particular needs and features of the work at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j20qHPviZCu-"
   },
   "source": [
    "## Different Learning Rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "5aE0S7Rxhdmz"
   },
   "outputs": [],
   "source": [
    "def build_model_lrate():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for learning rate 0.01:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "RMSE: 0.0343\n",
      "\n",
      "Results for learning rate 0.1:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "RMSE: 0.0399\n",
      "\n",
      "Results for learning rate 0.5:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "RMSE: 1.0178\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "learning_rate_names = ['0.01', '0.1', '0.5']\n",
    "\n",
    "for lr, name in zip(learning_rates, learning_rate_names):\n",
    "    print(f\"\\nResults for learning rate {name}:\")\n",
    "    model = build_model_lrate()\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=0)\n",
    "    evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KybmWwbzhdmz"
   },
   "source": [
    "### Discussion: Learning Rates\n",
    "In this challenge, we looked at how various learning rates affected a neural network's convergent and performing capabilities for a regression problem. Three different learning rates were tested: 0.01, 0.1, and 0.5. The outcomes and our observations are as follows:\n",
    "\n",
    "1. Learning Rate: 0.01\n",
    "* RMSE: 0.0343\n",
    "* Among the tested rates, the performance was highest with a learning rate of 0.01 with an RMSE of 0.0343. Because of its very low learning rate, the model was able to gradually and steadily converge by making little adjustments to the weights. By making sure that weight adjustments are made carefully, a conservative learning rate can improve model performance, as seen by the model's ability to minimise error without going overboard.\n",
    "\n",
    "2. Learning Rate: 0.1\n",
    "* RMSE: 0.0399\n",
    "* The model's performance somewhat declined with a learning rate of 0.1, yielding an RMSE of 0.0399. The model was able to converge faster due to the accelerated training process caused by the greater learning rate. But the model was probably unable to reach the same accuracy as the model with a learning rate of 0.01 because of the bigger step sizes in the weight updates, which led the model to bounce around the ideal solution. This shows that even while quicker convergence is feasible, if the learning rate is too high, it could result in more inaccuracy.\n",
    "\n",
    "3. Learning Rate: 0.5\n",
    "* RMSE: 1.0178\n",
    "* An RMSE of 1.0178 indicated poor performance at the maximum learning rate of 0.5. The model made unusually large weight changes as a result of this excessive learning rate, which resulted in instability and divergence. The model probably overshot the ideal solution several times, leading to a large error rate, rather than progressively getting closer to it. This result emphasises the risks associated with adopting an overly high learning rate, since it may impair the model's capacity to converge correctly and learn efficiently.\n",
    "\n",
    "### Analysis and Impact of Learning Rates\n",
    "One crucial hyperparameter that has a big impact on neural network performance and training dynamics is the learning rate. Below is a summary of the effects that were noticed:\n",
    "\n",
    "* Learning Rate: 0.01: The optimal compromise between accuracy and convergence speed was offered by this rate. The model was able to learn efficiently and achieve the lowest RMSE thanks to the smaller, more regulated updates.\n",
    "\n",
    "* Learning Rate: 0.1: This pace reduced the accuracy of the model but made training faster. This learning rate prevented the model from settling into the ideal answer, which led to larger updates and a marginally higher error.\n",
    "\n",
    "* Learning Rate: 0.5: This unnaturally high rate of learning resulted in instability and subpar work. The model diverged as a result of the big updates, showing that an excessively high learning rate might be harmful to learning.\n",
    "\n",
    "### Conclusion\n",
    "The significance of selecting a suitable learning rate when training neural networks is shown by this experiment. The best results were obtained with a lower learning rate (0.01), which balanced the need for precise weight modifications and steady convergence. Increased inaccuracy was a result of faster training at higher learning rates (0.1 and 0.5), with the maximum rate producing noticeable instability. These results demonstrate that, even while faster training may be preferred, it is critical to determine a learning rate that guarantees accuracy and efficiency while avoiding the dangers of divergence and overshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5lW2-IoZHwW"
   },
   "source": [
    "## Different Optimization Algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "iqwR-lVChegE"
   },
   "outputs": [],
   "source": [
    "def build_model_optimization():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for SGD optimizer:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "RMSE: 0.0266\n",
      "\n",
      "Results for Adam optimizer:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "RMSE: 0.0592\n",
      "\n",
      "Results for RMSprop optimizer:\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "RMSE: 0.0168\n"
     ]
    }
   ],
   "source": [
    "optimizers = [SGD(), Adam(), RMSprop()]\n",
    "optimizer_names = ['SGD', 'Adam', 'RMSprop']\n",
    "\n",
    "for optimizer, name in zip(optimizers, optimizer_names):\n",
    "    print(f\"\\nResults for {name} optimizer:\")\n",
    "    model = build_model_optimization()\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=0)\n",
    "    evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWF6uXlchegF",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Discussion: Optimization Algorithms\n",
    "In this challenge, we investigated the effects of several optimisation strategies on a neural network's performance on a regression problem. Three optimizers were put to the test: Adam (Adaptive Moment Estimation), RMSprop (Root Mean Square Propagation), and SGD (Stochastic Gradient Descent). The outcomes and our observations are as follows:\n",
    "\n",
    "1. SGD Optimizer:\n",
    "* RMSE: 0.0266\n",
    "* SGD stands for stochastic gradient descent, and it's one of the most straightforward and popular optimisation algorithms. It uses the gradients of the loss function to update the model's parameters. An RMSE of 0.0266 was obtained by the SGD optimizer in our experiment, suggesting good performance. SGD is a dependable option due to its simplicity, although it frequently needs the learning rate to be carefully adjusted and can occasionally converge slowly or become stuck in local minima.\n",
    "\n",
    "2. Adam Optimizer:\n",
    "* RMSE: 0.0592\n",
    "* Adam combines the benefits of AdaGrad and RMSprop, two further SGD enhancements. In addition to keeping a moving average of the gradients and squared gradients, it calculates adaptive learning rates for every parameter. Although the Adam optimizer is widely used and proven to be successful in other contexts, its RMSE of 0.0592 was higher in our work. This implies that the learning rate or other hyperparameters may not have been optimally fitted, which may have made Adam less successful in navigating the loss landscape for our particular regression problem.\n",
    "\n",
    "3. RMSprop Optimizer:\n",
    "* RMSE: 0.0168\n",
    "* By dividing the learning rate by an exponentially decreasing average of squared gradients, RMSprop is engineered to adjust the learning rate for every parameter. This enables faster convergence and aids in solving the vanishing gradient problem. RMSprop performed better in our experiment than both SGD and Adam, with the lowest RMSE of 0.0168. This shows how well it optimised the model for our regression job, indicating that RMSprop was able to better balance the efficient parameter updates.\n",
    "\n",
    "### Analysis and Impact of Optimization Algorithms\n",
    "The performance of neural networks during training is greatly impacted by the optimisation algorithm selection. Below is a summary of the effects that were noticed:\n",
    "\n",
    "##### SGD Optimizer:\n",
    "* With an RMSE of 0.0266, a good balance was obtained.\n",
    "* Its performance suggests that SGD can be a good optimizer, even for complicated workloads, with the right tuning.\n",
    "\n",
    "##### Adam Optimizer:\n",
    "* Adam's RMSE of 0.0592 indicates that his performance in this task was not optimal, even with its sophisticated methods for adjusting learning rates.\n",
    "* This implies that Adam might not always be the best option right out of the box and would need more careful adjustment or might not work well for all kinds of issues.\n",
    "\n",
    "##### RMSprop Optimizer:\n",
    "* With an RMSE of 0.0168, gave the best performance.\n",
    "* It achieved better outcomes in this assignment by navigating the loss landscape more skillfully and adjusting learning rates according to the magnitude of recent gradients.\n",
    "\n",
    "### Conclusion\n",
    "The significance of choosing the appropriate optimisation technique for neural network training is demonstrated by this experiment. SGD is still a strong and dependable option, however RMSprop performed better in our regression task and had the lowest RMSE. Adam did not fare as well in this particular circumstance, despite its widespread use, highlighting the fact that no single optimizer is always the best. Every optimisation algorithm has advantages and disadvantages, and the particulars of the given situation might affect how effective an algorithm is. For this reason, experimenting with various optimizers and fine-tuning their hyperparameters is essential to getting the best results for a particular assignment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
